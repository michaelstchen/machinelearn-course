\documentclass{article}

\usepackage{enumerate}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{graphicx}
\usepackage{listings}

\graphicspath{ {Images/} }

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\title{CS 189: Homework 5}
\author{Michael Stephen Chen\\ Kaggle Acct: michaelstchen \\SID: 23567341}

\begin{document}
\maketitle

\pagebreak

\section*{Implemetation Explanation}
\begin{enumerate}
  \item I implemented my \textbf{Decision Tree} so that it would take in a couple of hyperparameters: max depth, minimum leaf size, and number of features bagged per node. All three features effect the runtimes of my simulation (i.e. decreasing max depth, number of bagged features, and/or increasing min leaf size all decrease the amount of computation required to generate a tree). In the cases where a growing branch is terminated prematurely, either because it reached the max depth or min leaf size, then we set the label of the leaf by majority vote. At each node features are randomly chosen/bagged, without replacement of course. For quantitative features, we do not test each possible split but instead choose up to 4 equally spaces splits based on the range of the feature's values. This is because it would be computationally intensive to check all possible splits, especially for some of the quantitative features in the census data (i.e. ``capital-gain'' for which each sample probably has a unique value).
  
  \item For my \textbf{Random Forests} implentation, I didn't have to make any additional moifications to my DecisionTree class because I had already implemented feature bagging at every node. Essentially I maintained a list of $T=30$ trees that were all trained on different data bags that contains randomly chose samples, with replacement, from the overall training data set. I then use all $T$ trees to predict each data point, and take the rounded average as my final prediction. Compared to the single trees, I generally increased the max depth and decreased the min leaf size for my forest trees because I wanted a lower possible bias for each tree. The consequence of this is that each individual tree will have a larger variance, but that is hopefully offset by taking the average of multiple trees. For feature bagging I decided to bag $\sqrt{d}$ features at each node, where $d$ is the total number of features. This decision was made based on Shewchuck's suggestion in lecture; he noted that this generally worked well in practice for decision tree classifiers. 
\end{enumerate}

\section*{Spam Report}
\begin{enumerate}
  \item I used a custom featurizer, which is just the one given for previous homeworks with additional words added (e.g. ``viagra'', ``re :'', etc.). I added 16 additional word features in total, bringing the number of featuers per sample up to 48. For a full list of additional words, see \textit{featurize.py}.
 
  \item When using a sole decision tree, I only trained on 2/3 of the samples and validated on the rest of the 1/3. At each node, I considered all 48 features, so no feature bagging. Depths were limited at 10 and leaf size was limited at 10 (to both prevent overfitting and lower runtimes). The validation accuracy was \textbf{0.8485}.

     For my random forests implemetation, I again trained on 2/3, 3448, of the samples and validated on the rest of the 1/3. Each decision tree was trained using a randomly chosen (with replacement)``bag'' of 3448 samples (so the same as the number of samples in the training set). At each node, a random ``bag'' of 7 features was used, corresponding to roughly the square root of the total number of features (48). Depths were limited at 50 and leaf size was limited at 10. My forest consisted of 30 trees. With this setup, the validation set was classified correctly with a rate of \textbf{0.8741}.

   \item For the Kaggle competiton, I decided to use the random forest impelementation with the same hyperparameters I defined above. My best Kaggle score (random forests) was \textbf{0.80247}. In the interest of time I merely ran the script a couple times to get an idea of what hyperaparameter values work. I probably could've probably bumped up my score if I had more rigorously tuned my parameters using cross-validation.

   \item I chose a random sample that was classified as 0 (ham):
     \begin{enumerate}[i.]
       \item ``cc :'' $<$ 0.5
       \item ``!'' $<$ 0.5
       \item ``\%'' $<$ 0.5
       \item ``meter'' $<$ 0.5
       \item ``('' $<$ 0.5
       \item ``www'' $<$ 0.5
       \item ``sex'' $<$ 0.5
       \item ``\&'' $<$ 0.5
       \item ``;'' $<$ 0.5
       \item ``volumes'' $>$ 0.5
       \item label = 0
     \end{enumerate}

   \item For my random forest with 30 trees, the root split counts (``feature'', ``value split on'') were as follows:
     \begin{enumerate}[i.]
       \item (``!'', 0.5) - \textbf{4 trees}
       \item (``meter'', 0.5), (``cc :'', 0.5) - \textbf{3 trees}
       \item (``featured'', 0.5), (``\&'', 0.5), (``re :'', 1.5), (``sex'', 0.5), (``money'', 0.5), (``cialis'', 0.5), (``microsoft'', 0.5), (``\%'', 0.5) - \textbf{2 trees}
       \item (``creative'', 0.5), (``www'', 0.5), (``viagra'', 0.5), (``volumes'', 0.5) - \textbf{1 tree}
     \end{enumerate}

\end{enumerate}

\section*{Census Report}
\begin{enumerate}
  \item I used the features specified in the provided data files, where each sample point had 15 different features. I replaced missing feature values with the mode of the respective feature. Initially I tried to do this using the sklearn.preprocessing.Imputer, however I ran into issues so I instead opted to manually scan the data set (see \textit{featurize\_census.py}). I then used sklearn.feature\_extraction.DictVectorizer to one-hot encode the categorical features. This resulted in a data matrix consisting of 105 features.
  
  \item When using a sole decision tree, I only trained on 2/3 of the samples, 21816, and validated on the rest. At each node, all 105 features were taken into account. Depths were limited at 10 and leaf size was limited at 100 (to both prevent overfitting and lower runtimes). The validation accuracy was \textbf{0.8534}

    For my random forests implemetation, I split up the available data 50/50 into training data and validation data (so 16362 samples each). Each decision tree was trained using a randomly chosen ``bag'' of 16362 training samples, with replacement. At each node, a random ``bag'' of 10 features was used, corresponding to roughly the square root of the total number of features (105). Depths were limited at 50 and leaf size was limited at 10. My forest only consisted of 30 trees. With this setup, the validation set was classified correctly with a rate of \textbf{0.8657}.
    
    For the Kaggle competiton, my best Kaggle score (random forests) was \textbf{0.75686}. For comparison my Kaggle score using the sole decision tree was \textbf{0.6805}. In the interest of time I merely ran the script a couple times to get an idea of what hyperaparameter values work. I probably could've probably bumped up my score if I had more rigorously tuned my parameters using cross-validation.
    
  \item The example data point we will classify is: \{age=72,workclass=Private,fnlwgt=156310,education=10th,education-num=6,marital-status=Married-civ-spouse,occupation=Other-service,relationship=Husband,race=White,sex=Male,\\
    capital-gain=2414,capital-loss=0,hours-per-week=12,native-country=United-States,label=0\}
    \begin{enumerate}[i.]
      \item marital-status=Married-civ-spouse $>$ 0.5
      \item occupation=Prof-specialty $<$ 0.5
      \item education-num $<$ 12.5
      \item age $>$ 30.5
      \item capital-gain $<$ 5095.5
      \item education=Some-college $<$ 0.5
      \item hours-per-week $<$ 40.5
      \item occupation=Adm-clerical $<$ 0.5
      \item education=Assoc-acdm $<$ 0.5
      \item age $>$ 66.5
      \item label = 0
    \end{enumerate}


  \item For my random forest with 30 trees, the root split counts (``feature'', ``value split on'') were as follows:
    \begin{enumerate}[i.]
      \item (relationship=Husband, 0.5) - \textbf{6 trees}
      \item (age, 27.5), (education-num, 12.5), (marital-status=Married-civ-spouse, 0.5), (sex=Female, 0.5) - \textbf{3 trees}
      \item (capital-gain, $> 5095.5$), (marital-status=Never-married, 0.5), (relationship=own-child, 0.5), (relationship=Unmarried, 0.5) - \textbf{2 trees}
      \item (education=Bachelors, 0.5), (education=HS-grad, 0.5), (workclass=Private, 0.5) - \textbf{1 tree}
    \end{enumerate}
\end{enumerate}

\section*{Regarding My High Validation Accuracy But Relatively Low Kaggle Scores}

For the census random forests, something interesting of note is that of the total 32724 provided samples, only $24.13\%$ are labeled with a 1 ($>\$50,000$). Similarly in the validation subset $24.11\%$ were labeled a 1, however our predicted labels were $19.23\%$ labeled 1. The predicted labels for the test set were also $18.9\%$ labeled 1. I believe that the skew in the data toward labels of 0 is mostly why my kaggle scores are relatively low compared to my validation accuracy. If there were a way to somehow factor in prior probabilities then maybe I could have addressed this issue.

However this doesn't explain the dsicrepancy between my single tree and random forest Kaggle scores for the census data. They had similar validation accuracies but widely differing Kaggle accuracies.

\pagebreak

\section*{Appendix: featurize.py}
\lstinputlisting[language=python]{featurize.py}

\section*{Appendix: featurize\_census.py}
\lstinputlisting[language=python]{featurize_census.py}

\section*{Appendix: DecisionTree.py}
\lstinputlisting[language=python]{DecisionTree.py}

\section*{Appendix: spam\_simple.py}
\lstinputlisting[language=python]{spam_simple.py}

\section*{Appendix: spam\_forest.py}
\lstinputlisting[language=python]{spam_forest.py}

\section*{Appendix: census\_simple.py}
\lstinputlisting[language=python]{census_simple.py}

\section*{Appendix: census\_forest.py}
\lstinputlisting[language=python]{census_forest.py}


\end{document}
