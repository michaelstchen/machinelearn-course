\documentclass{article}

\usepackage{enumerate}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{graphicx}
\usepackage{listings}

\graphicspath{ {Images/} }

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\title{CS 189: Homework 5}
\author{Michael Stephen Chen\\ Kaggle Acct: michaelstchen \\SID: 23567341}

\begin{document}
\maketitle

\pagebreak

\section*{Implemetation Explanation}
\begin{enumerate}
  \item I implemented my \textbf{Decision Tree} so that it would take in a couple of hyperparameters: max depth, minimum leaf size, and number of features bagged per node. All three features  
  
  \item For my \textbf{Random Forests}, 
\end{enumerate}

\section*{Spam Report}
\begin{enumerate}
  \item I used a custom featurizer, which is just the one given for previous homeworks with additional words added (e.g. ``viagra'', ``re :'', etc.). I added 16 additional word features in total, bringing the number of featuers per sample up to 48. For a full list of additional words, see \textit{featurize.py}.
 
  \item When using a sole decision tree, I only trained on 2/3 of the samples and validated on the rest of the 1/3. At each node, I considered all 48 features, so no feature bagging. Depths were limited at 10 and leaf size was limited at 10 (to both prevent overfitting and lower runtimes). The validation accuracy was \textbf{0.8485}.

     For my random forests implemetation, I again trained on 2/3, 3448, of the samples and validated on the rest of the 1/3. Each decision tree was trained using a randomly chosen (with replacement)``bag'' of 3448 samples (so the same as the number of samples in the training set). At each node, a random ``bag'' of 7 features was used, corresponding to roughly the square root of the total number of features (48). This decision was made based on Shewchuck's general suggestion in lecture. Depths were limited at 50 and leaf size was limited at 10. I allowed a deeper depth for my forest trees because I wanted to reduce bias at the cost of variance, because hopefully having multiple trees and averaging results will reduce the variance. My forest consisted of 30 trees. With this setup, the validation set was classified correctly with a rate of \textbf{0.8741}.

   \item For the Kaggle competiton, I decided to use the random forest impelementation with the same hyperparameters I defined above. My best Kaggle score (random forests) was \textbf{0.80247}. In the interest of time I merely ran the script a couple times to get an idea of what hyperaparameter values work. I probably could've probably bumped up my score if I had more rigorously tuned my parameters using cross-validation.

   \item I chose a random sample that was classified as 0 (ham):
     \begin{enumerate}[i.]
       \item ``cc :'' $<$ 0.5
       \item ``!'' $<$ 0.5
       \item ``\%'' $<$ 0.5
       \item ``meter'' $<$ 0.5
       \item ``('' $<$ 0.5
       \item ``www'' $<$ 0.5
       \item ``sex'' $<$ 0.5
       \item ``\&'' $<$ 0.5
       \item ``;'' $<$ 0.5
       \item ``volumes'' $>$ 0.5
       \item label = 0
     \end{enumerate}

   \item 

\end{enumerate}

\section*{Census Report}
\begin{enumerate}
  \item I used the features specified in the provided data files, where each sample point had 15 different features. I replaced missing feature values with the mode of the respective feature. Initially I tried to do this using the sklearn.preprocessing.Imputer, however I ran into issues so I instead opted to manually scan the data set (see \textit{featurize\_census.py}). I then used sklearn.feature\_extraction.DictVectorizer to one-hot encode the categorical features. This resulted in a data matrix consisting of 105 features.
  
  \item When using a sole decision tree, I only trained on 12000 samples and validated on the rest of the 20724 samples. At each node, a random ``bag'' of 35 features was used (this was imposed primarily because the runtimes were too long for the full 105 features per node). Depths were limited at 10 and leaf size was limited at 10 (to both prevent overfitting and lower runtimes). The validation accuracy was \textbf{0.8425}

    For my random forests implemetation, I split up the available data 50/50 into training data and validation data (so 16362 samples each). Each decision tree was trained using a randomly chosen (with replacement)``bag'' of 8000 samples of the 16362 training samples. At each node, a random ``bag'' of 10 features was used, corresponding to roughly the square root of the total number of features (105). This decision was made based on Shewchuck's general suggestion in lecture. Depths were limited at 15 and leaf size was limited at 10. I allowed a deeper depth for my forest trees because I wanted to reduce bias at the cost of variance, because hopefully having multiple trees and averaging results will reduce the variance. My forest only consisted of 11 trees (I limited the number of trees becuase it was taking around 2min to build each tree). With this setup, the validation set was classified correctly with a rate of \textbf{0.8530}.
    
    For the Kaggle competiton, I decided to use the random forest impelementation with the same hyperparameters I defined above. My best Kaggle score (random forests) was \textbf{0.75686}. For comparison my Kaggle score using the sole decision tree was \textbf{0.6805}. In the interest of time I merely ran the script a couple times to get an idea of what hyperaparameter values work. I probably could've probably bumped up my score if I had more rigorously tuned my parameters using cross-validation.
    
  \item The example data point we will classify is: \{age=72,workclass=Private,fnlwgt=156310,education=10th,education-num=6,marital-status=Married-civ-spouse,occupation=Other-service,relationship=Husband,race=White,sex=Male,capital-gain=2414,capital-loss=0,hours-per-week=12,native-country=United-States,label=0\}
    \begin{enumerate}[i.]
      \item marital-status=Married-civ-spouse $>$ 0.5
      \item occupation=Prof-specialty $<$ 0.5
      \item education-num $<$ 12.5
      \item age $>$ 30.5
      \item capital-gain $<$ 5095.5
      \item education=Some-college $<$ 0.5
      \item hours-per-week $<$ 40.5
      \item occupation=Adm-clerical $<$ 0.5
      \item education=Assoc-acdm $<$ 0.5
      \item age $>$ 66.5
      \item label = 0
    \end{enumerate}


  \item For my random forest with 11 trees, the root split counts (``feature'', ``value split on'') were as follows:
    \begin{enumerate}[i.]
      \item (education-num, 12.5), (relationship=Husband, 0.5), (relationship=Not-in-family, 0.5), (sex=Male, 0.5) - \textbf{2 trees}
      \item (age, 29.5), (marital-status=Divorced, 0.5), (marital-status=Married-civ-spouse, 0.5) - \textbf{1 tree}
    \end{enumerate}
    For the sake of this problem I also ran my random forest script with 30 trees but each tree was capped at a depth of 5 so that the runtime would not be too long. The accuracy was understandably poor given the shallow-nature of these trees (high bias), the Kaggle score was 0.68053. Of course the small depth should not effect the choice of the root node. The root counts for this run were as follows:
    \begin{enumerate}[i.]
      \item (relationship=Husband, 0.5) - \textbf{6 trees}
      \item (age, 27.5), (education-num, 12.5), (marital-status=Married-civ-spouse, 0.5), (sex=Female, 0.5) - \textbf{3 trees}
      \item (capital-gain, $> 5095.5$), (marital-status=Never-married, 0.5), (relationship=own-child, 0.5), (relationship=Unmarried, 0.5) - \textbf{2 trees}
      \item (education=Bachelors, 0.5), (education=HS-grad, 0.5), (workclass=Private, 0.5) - \textbf{1 tree}
    \end{enumerate}
\end{enumerate}

\section*{Regarding My High Validation Accuracy But Relatively Low Kaggle Scores}

For the census random forests, something interesting of note is that of the total 32724 provided samples, only $24.13\%$ are labeled with a 1 ($>\$50,000$). Similarly in the validation subset $24.11\%$ were labeled a 1, however our predicted labels were $19.23\%$ labeled 1. The predicted labels for the test set were also $18.9\%$ labeled 1.


\end{document}
